{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify\n",
    "\n",
    "## PySpark Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparkify is a fictional music-streaming company, and in this notebook, I'm going to analyze Sparkify's streaming data to predict customers that are likely to churn. Udacity provided two separate datasets, a mini-version (128MB), which was used in this notebook, and a larger version (12GB), which was used in an AWS EMR cluster.\n",
    "\n",
    "Check out the accompanying Medium Article (linked in my [Github Repo](https://github.com/pdeguzman96/sparkify)) for a write up and reflection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "One of the most important concerns for companies with subscription-based business models is customer churn. Customers downgrade or discontinue service for various reasons, and the service provider often cannot know when or why customers leave until they leave!\n",
    "\n",
    "If we can reliably predict whether a customer is likely to churn, we have the chance to retain these customers by intervening with promotions, communicating new features, etc. This is a *proactive* approach to retaining customers, as opposed to a *reactive* approach of getting back lost customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import count, col, udf, desc, max as Fmax, lag, struct, date_add, sum as Fsum, \\\n",
    "                        datediff, date_trunc, row_number, when, coalesce, avg as Favg\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('mini_sparkify_event_data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis & Initial Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_ps_df(df):\n",
    "    '''\n",
    "    Print shape of PySpark DataFrame\n",
    "    '''\n",
    "    print(f'DF Shape: ({df.count()},{len(df.columns)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_ps_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the contents of a few columns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('level').dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('status').dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('page').agg(count(col('userId')).alias('count_visits')).show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like `Cancellation Confirmation` and `Downgrade` are good indicators of churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a look at the userIds\n",
    "df.select('userId').sort('userId').dropDuplicates().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the blank userIds\n",
    "df = df.where(col('userId')!='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a look at location\n",
    "df.select('location').sort('location').dropDuplicates().take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location roughly looks like we may be able to parse the state by taking the last two character strings\n",
    "get_state = udf(lambda x: x[-2:])\n",
    "df = df.withColumn('state',get_state(col('location')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('state').dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listens by state\n",
    "df.filter(col('page')=='NextSong') \\\n",
    "    .groupBy('state') \\\n",
    "    .agg(count('userId').alias('count')) \\\n",
    "    .sort(desc('count')) \\\n",
    "    .show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique users\n",
    "df.select(['userId']).dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique users by state\n",
    "df.filter(col('page')=='NextSong') \\\n",
    "    .dropDuplicates(['userId']) \\\n",
    "    .groupBy('state') \\\n",
    "    .agg(count('userId').alias('count')) \\\n",
    "    .sort(desc('count')) \\\n",
    "    .show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like most of Sparkify's listeners live in California, and we don't have all 50 states represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Date Data\n",
    "Now feature engineering more granularity from the timestamp column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some functions to help pull hour, day, month, and year\n",
    "get_hour = udf(lambda x: datetime.datetime.fromtimestamp(x/1000).hour,IntegerType())\n",
    "get_day = udf(lambda x: datetime.datetime.fromtimestamp(x/1000).day,IntegerType())\n",
    "get_month = udf(lambda x: datetime.datetime.fromtimestamp(x/1000).month,IntegerType())\n",
    "get_year = udf(lambda x: datetime.datetime.fromtimestamp(x/1000).year,IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the columns\n",
    "df = df \\\n",
    "    .withColumn('hour',get_hour(col('ts'))) \\\n",
    "    .withColumn('day',get_day(col('ts'))) \\\n",
    "    .withColumn('month',get_month(col('ts'))) \\\n",
    "    .withColumn('year',get_year(col('ts')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also creating a feature with the PySpark DateType() just in case\n",
    "get_date = udf(lambda x: datetime.datetime.fromtimestamp(x/1000),DateType())\n",
    "df = df.withColumn('date',get_date(col('ts')))\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now aggregating by date data my hour to see if there are any trends.\n",
    "df.filter(col('page')=='NextSong').groupBy('hour').agg(count('userId')).sort('hour').show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating again by day\n",
    "df.filter(col('page')=='NextSong').groupBy('day').agg(count('userId')).sort('day').show(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Month\n",
    "df.filter(col('page')=='NextSong').groupBy('month').agg(count('userId')).sort('month').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Year\n",
    "df.filter(col('page')=='NextSong').groupBy('year').agg(count('userId')).sort('year').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transferring the above date analysis onto a Pandas DF\n",
    "df_pd = df.filter(col('page')=='NextSong').select(['hour','day','month','userId']).toPandas()\n",
    "df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "\n",
    "plt.subplot(131)\n",
    "sns.countplot(x='hour',data=df_pd)\n",
    "plt.title('Events by Hour')\n",
    "\n",
    "plt.subplot(132)\n",
    "sns.countplot(x='day',data=df_pd)\n",
    "plt.title('Events by Day')\n",
    "\n",
    "plt.subplot(133)\n",
    "sns.countplot(x='month',data=df_pd)\n",
    "plt.title('Events by Month')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we only have data from 2018 from the smaller dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at User Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a days with consecutive listens feature\n",
    "It may be useful to keep a running tally of consecutive days a user listens to a song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column containing 1 if the event was a \"NextSong\" page visit or 0 otherwise\n",
    "listen_flag = udf(lambda x: 1 if x=='NextSong' else 0, IntegerType())\n",
    "df = df.withColumn('listen_flag',listen_flag('page'))\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a second table where I will create this feature, then join it back to the main table later\n",
    "df_listen_day = df.select(['userId','date','listen_flag']) \\\n",
    "                .groupBy(['userId','date']) \\\n",
    "                .agg(Fmax('listen_flag')).alias('listen_flag').sort(['userId','date'])\n",
    "df_listen_day.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a window partitioned by User and ordered by date\n",
    "window = Window \\\n",
    "        .partitionBy('userId') \\\n",
    "        .orderBy(col('date'))\n",
    "\n",
    "# Using the above defined window and a lag function to create a previous day column\n",
    "df_listen_day = df_listen_day \\\n",
    "                    .withColumn('prev_day',lag(col('date')) \\\n",
    "                    .over(window))\n",
    "df_listen_day.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a udf to compare one date to another\n",
    "def compare_date_cols(x,y):\n",
    "    '''\n",
    "    Compares x to y. Returns 1 if different\n",
    "    '''\n",
    "    if x != y:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "date_group = udf(compare_date_cols, IntegerType())\n",
    "\n",
    "# Creating another window partitioned by userId and ordered by date\n",
    "windowval = (Window.partitionBy('userId').orderBy('date')\n",
    "             .rangeBetween(Window.unboundedPreceding, 0))\n",
    "\n",
    "df_listen_day = df_listen_day \\\n",
    "                        .withColumn( \\\n",
    "                            'date_group',\n",
    "                             date_group(col('date'), date_add(col('prev_day'),1)) \\\n",
    "                                    # The above line checks if current day and previous day +1 day are equivalent\n",
    "                                        # If They are equivalent (i.e. consecutive days), return 1\n",
    "                            ) \\\n",
    "                        .withColumn( \\\n",
    "                             'days_consec_listen',\n",
    "                             Fsum('date_group').over(windowval)) \\\n",
    "                        .select(['userId','date','days_consec_listen'])\n",
    "                                    # The above lines calculate a running total summing consecutive listens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining this intermediary table back into the original DataFrame\n",
    "df = df.join(other=df_listen_day,on=['userId','date'],how='left')\n",
    "shape_ps_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.where(col('page')=='NextSong') \\\n",
    "    .select(['userId','date','days_consec_listen']) \\\n",
    "    .sort(['userId','date']) \\\n",
    "    .dropDuplicates(['userId','date']) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a days since last listen feature\n",
    "It my also be useful to use this to measure inactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating a few columns and taking the max aggregation to effectively remove duplicates\n",
    "df_listen_day = df.select(['userId','date','listen_flag']) \\\n",
    "                .groupBy(['userId','date']) \\\n",
    "                .agg(Fmax('listen_flag')).alias('listen_flag').sort(['userId','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listen_day.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-stating the window\n",
    "windowval = Window.partitionBy('userId').orderBy('date')\n",
    "\n",
    "# Calculate difference (via datediff) between current date and previous date (taken with lag), and filling na's with 0\n",
    "df_last_listen = df_listen_day.withColumn('days_since_last_listen',\n",
    "                                            datediff(col('date'),lag(col('date')).over(windowval))) \\\n",
    "                            .fillna(0,subset=['days_since_last_listen']) \\\n",
    "                            .select(['userId','date','days_since_last_listen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining back results\n",
    "df = df.join(df_last_listen,on=['userId','date'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Listens By Month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listens by month can also be a useful indicator of consistent user activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Window \n",
    "windowval = Window.partitionBy('userId').orderBy(date_trunc('month',col('date')))\n",
    "\n",
    "# Creating separate intermediary DF. Using row_number() on each listen within each month to count monthly listens\n",
    "df_running_listens = df \\\n",
    "                    .where(col('listen_flag')==1) \\\n",
    "                    .withColumn('running_listens_mon',row_number().over(windowval)) \\\n",
    "                    .select(['userId','ts','running_listens_mon','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining back into main DF\n",
    "df = df.join(df_running_listens.select(['userId','ts','running_listens_mon']),\n",
    "                                       on=['userId','ts'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(['userId','date','page','running_listens_mon']).sort(['userId','ts']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dealing with Missing Values in running monthly listens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method creates a lot of null values. Let's see how many nulls we have..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col('running_listens_mon').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by userId and timestamp\n",
    "df = df.sort(['userId','ts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a window partitioned by userId and ordered by timestamp\n",
    "windowval = Window.partitionBy(col('userId')).orderBy(col('ts'))\n",
    "\n",
    "# Creating a lag of the new running listens column\n",
    "running_listens_lag = lag(df['running_listens_mon']).over(windowval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When a null value is found, fill it with the previous value. \n",
    "    # This effectively frontfills null values with valid values that immediately precede it\n",
    "df = df.withColumn('running_listens_mon_fill', \n",
    "              when(col('running_listens_mon').isNull(),running_listens_lag) \\\n",
    "                .otherwise(col('running_listens_mon')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recounting nulls\n",
    "df.where(col('running_listens_mon_fill').isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still have null values that have to be filled. Re-running the lag as a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_null = df.where(col('running_listens_mon_fill').isNull()).count()\n",
    "n_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while n_null > 0:\n",
    "    # Re-creating a lag column based on the filled values\n",
    "    running_listens_lag = lag(df['running_listens_mon_fill']).over(windowval)\n",
    "    \n",
    "    # Replacing 'running_listens_mon_fill' with new filled values\n",
    "    df = df.withColumn('running_listens_mon_fill', \n",
    "                  when(col('running_listens_mon_fill').isNull(),running_listens_lag) \\\n",
    "                    .otherwise(col('running_listens_mon_fill')))\n",
    "\n",
    "    n_null = df.where(col('running_listens_mon_fill').isNull()).count()\n",
    "    i += 1\n",
    "    print(f'Loop {i}\\nNull values left: {n_null}')\n",
    "    \n",
    "    if i > 5:\n",
    "        print('Breaking loop to save computation time. Filling remaining null values with 0.')\n",
    "        df = df.fillna(0,subset=['running_listens_mon_fill'])\n",
    "\n",
    "print(f'Done.\\nNumber of null values remaining: {n_null}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flagging Based on Other Page Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating udf's to flag whenever a user visits each particular page\n",
    "thU_flag = udf(lambda x: 1 if x=='Thumbs Up' else 0, IntegerType())\n",
    "thD_flag = udf(lambda x: 1 if x=='Thumbs Down' else 0, IntegerType())\n",
    "err_flag = udf(lambda x: 1 if x=='Error' else 0, IntegerType())\n",
    "addP_flag = udf(lambda x: 1 if x=='Add to Playlist' else 0, IntegerType())\n",
    "addF_flag = udf(lambda x: 1 if x=='Add Friend' else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the flag columns\n",
    "df = df.withColumn('thU_flag',thU_flag('page')) \\\n",
    "        .withColumn('thD_flag',thD_flag('page')) \\\n",
    "        .withColumn('err_flag',err_flag('page')) \\\n",
    "        .withColumn('addP_flag',addP_flag('page')) \\\n",
    "        .withColumn('addF_flag',addF_flag('page'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will consider a page visit to `Cancellation Confirmation` or  `Downgrade` churn, which will be denoted by 1 in a `Churn` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_churn(x):\n",
    "    '''\n",
    "    INPUT\n",
    "    x: Page\n",
    "    \n",
    "    OUTPUT\n",
    "    Returns 1 if an instance of Churn, else returns 0\n",
    "    '''\n",
    "    if x=='Cancellation Confirmation':\n",
    "        return 1\n",
    "    elif x=='Downgrade':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Creating udf\n",
    "udf_label_churn = udf(label_churn, IntegerType())\n",
    "# Creating column\n",
    "df = df.withColumn('Churn',udf_label_churn(col('page')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at average of the running listens per month by churn\n",
    "df.groupBy('Churn').agg(Favg(col('running_listens_mon'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating User Aggregations\n",
    "Using the features I engineered above, I'll aggregate these for each user.\n",
    "- For the metrics I calculated using Window functions (i.e. `running_listens_mon` or `days_consec_listen`), I'm taking the max\n",
    "    - These represent *most listens in one month* and *most consecutive days spent listening to music* respectively\n",
    "- For the flag metrics (i.e. `listen_flag` or `thU_flag`), I'm taking the total sum\n",
    "    - These represent *total listens* and *total thumbs ups* respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_listens_user = df.groupBy('userId') \\\n",
    "            .agg(Fmax(col('running_listens_mon_fill')).alias('most_listens_one_month'),\n",
    "                 Fmax(col('days_since_last_listen')).alias('most_days_since_last_listen'),\n",
    "                 Fmax(col('days_consec_listen')).alias('most_days_consec_listen'),\n",
    "                 Fsum(col('listen_flag')).alias('total_listens'),\n",
    "                 Fsum(col('thU_flag')).alias('total_thumbsU'),\n",
    "                 Fsum(col('thD_flag')).alias('total_thumbsD'),\n",
    "                 Fsum(col('err_flag')).alias('total_err'),\n",
    "                 Fsum(col('addP_flag')).alias('total_add_pl'),\n",
    "                 Fsum(col('addF_flag')).alias('total_add_fr')\n",
    "                )\n",
    "df_listens_user.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking At User Session Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another potentially useful indicator is the extent to which users behave within each session. Below I'm first taking the total sum of each flag behavior (i.e. `listen_flag`) within each session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess = df.select(['userId','sessionId','listen_flag','thU_flag','thD_flag','err_flag','addP_flag','addF_flag']) \\\n",
    "            .groupBy(['userId','sessionId']) \\\n",
    "            .agg(Fsum(col('listen_flag')).alias('sess_listens'),\n",
    "                 Fsum(col('thU_flag')).alias('sess_thU'),\n",
    "                 Fsum(col('thD_flag')).alias('sess_thD'),\n",
    "                 Fsum(col('err_flag')).alias('sess_err'),\n",
    "                 Fsum(col('addP_flag')).alias('sess_addP'),\n",
    "                 Fsum(col('addF_flag')).alias('sess_addF'))\n",
    "df_sess.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm taking the average over all each user's session to get a sense of how a user tends to behave in one session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sess_agg = df_sess.groupBy('userId') \\\n",
    "                .agg(Favg(col('sess_listens')).alias('avg_sess_listens'),\n",
    "                    Favg(col('sess_thU')).alias('avg_sess_thU'),\n",
    "                    Favg(col('sess_thD')).alias('avg_sess_thD'),\n",
    "                    Favg(col('sess_err')).alias('avg_sess_err'),\n",
    "                    Favg(col('sess_addP')).alias('avg_sess_addP'),\n",
    "                    Favg(col('sess_addF')).alias('avg_sess_addF'))\n",
    "df_sess_agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to take the approach of creating a user-metric matrix (users by calculated metrics) and using this as the basis for training and predicting churn rather than using the original provided transactional streaming data.\n",
    "\n",
    "I believe this is a good approach to this problem since this heavily simplifies the training and prediction process. If this were to be implemented in practice, a streaming pipeline would be required to feed into the user-metric matrix, and a model would predict churn from this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUserMatrix = df.groupBy('userId').agg(Fmax(col('gender')).alias('gender')\n",
    "                                             ,Fmax(col('churn')).alias('churn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the heavy class imbalance\n",
    "dfUserMatrix.groupBy('churn').agg(count('*')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUserMatrix = dfUserMatrix.join(df_listens_user,['userId']).join(df_sess_agg,['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_ps_df(dfUserMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUMpd = dfUserMatrix.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUMpd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fields = ['avg_sess_listens',\n",
    "          'avg_sess_thU',\n",
    "          'avg_sess_thD',\n",
    "          'avg_sess_err',\n",
    "          'avg_sess_addP',\n",
    "          'avg_sess_addF',\n",
    "          'most_days_since_last_listen',\n",
    "          'most_days_consec_listen']\n",
    "aggs = ['mean','std']\n",
    "agg_dict = {k:agg for k,agg in zip(fields,[aggs]*len(fields))}\n",
    "agg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfUM_agged = dfUMpd.groupby('churn').agg(agg_dict)\n",
    "dfUM_agged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing mean values and storing them as tuples with label as first element in tuple\n",
    "means = []\n",
    "for i in dfUM_agged.index.values:\n",
    "    for field in fields:\n",
    "        means.append((i,dfUM_agged.iloc[i][field]['mean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the means list\n",
    "means[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUMpd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearly we have many more churned users than non-churned users\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(x='churn',hue='gender', data=dfUMpd)\n",
    "plt.title('Churned vs Non-Churned Users')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Churn')\n",
    "plt.xticks([0,1],['No','Yes'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting average activity per session for each group\n",
    "y_churn = [x[1] for x in means if x[0]==1]\n",
    "y_nochurn = [x[1] for x in means if x[0]==0]\n",
    "\n",
    "x = fields\n",
    "N = len(fields)\n",
    "ind = np.arange(N)\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(ind, y_churn, width, label='Churn')\n",
    "plt.bar(ind+width, y_nochurn, width, label='No Churn')\n",
    "\n",
    "plt.ylabel('Average')\n",
    "plt.title('Comparing Churned users to Non-Churned Users')\n",
    "plt.xticks(ind+width/2, fields, rotation=20)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping Data For Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark requires the data to be stored in a very particular format.\n",
    "- It needs numbers only\n",
    "- All features are in one column in vector format\n",
    "- All labels are in their own column\n",
    "\n",
    "Here's where I'll set all that up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing gender to turn a categorical feature into a binary feature\n",
    "gender_indexer = StringIndexer(inputCol='gender',outputCol='gender_indexed')\n",
    "fitted_gender_indexer = gender_indexer.fit(dfUserMatrix)\n",
    "dfModel = fitted_gender_indexer.transform(dfUserMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModel.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the that we want to vectorize in a list\n",
    "features = [col for col in dfModel.columns if col not in ('userId','gender','churn')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the features\n",
    "assembler = VectorAssembler(inputCols=features,\n",
    "                            outputCol='features')\n",
    "dfModelVec = assembler.transform(dfModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelVec = dfModelVec.select(col('features'),col('Churn').alias('label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may not be necessary to scale features based on the chosen algorithm. Tree-based algorithms are not sensitive to the scale of the features. However, algorithms like SVC and Logistic Regression perform poorly when features widely differ in scale.\n",
    "\n",
    "I know I'd like to try Logistic Regression, so I'll standardize features here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling to mean 0 and unit std dev\n",
    "scaler = StandardScaler(inputCol='features', outputCol='features_scaled', withMean=True, withStd=True)\n",
    "scalerModel = scaler.fit(dfModelVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelVecScaled = scalerModel.transform(dfModelVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMain = dfModelVecScaled.select(col('features_scaled').alias('features'),col('label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split - 80% train and 20% test\n",
    "df_train, df_test = dfMain.randomSplit([0.8,0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the class imbalance in the dataset (many more churned users than non-churned users) and simple binary classification, I decided to use accuracy and f-1 score because they're easy to interpret. Accuracy describes how often our model is correct regardless of the type of errors it makes, and F-1 score balances the tradeoff between precision (how often is the model correct over every \"positive\" prediction) and recall (how many of the total \"positive\" instances were identified correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(model,df_train=df_train, df_test=df_test):\n",
    "    '''\n",
    "    Used to train and evaluate a SparkML model based on accuracy and f-1 score\n",
    "    \n",
    "    INPUT\n",
    "    model: ML Model to train\n",
    "    df_train: DataFrame with data\n",
    "    \n",
    "    OUTPUT\n",
    "    None\n",
    "    '''\n",
    "    print(f'Training {model}...')\n",
    "    # Instantiating Evaluators\n",
    "    acc_evaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "    f1_evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
    "    \n",
    "    # Training and predicting with model\n",
    "    modelFitted = model.fit(df_train)\n",
    "    results = modelFitted.transform(df_test)\n",
    "    \n",
    "    # Calculating metrics\n",
    "    acc = acc_evaluator.evaluate(results)\n",
    "    f1 = f1_evaluator.evaluate(results)\n",
    "    \n",
    "    print(f'{str(model):<35s}Accuracy: {acc:<4.2%} F-1 Score: {f1:<4.3f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbitrarily picked these three algorithms to try\n",
    "lr = LogisticRegression(maxIter=30)\n",
    "gbt = GBTClassifier()\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the results from an initial evaluation pass through each of the three selected algorithms. We'll proceed with tuning `GBTClassifier` since it resulted in the highest Accuracy and F-1 Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model in [lr, gbt, rf]:\n",
    "    train_eval(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Tuning The Model\n",
    "\n",
    "Because of the very few data points that we have, it would be beneficial to train our final model using K-Fold cross validation, which is automatically done with the `CrossValidator` along with a Grid Search using `ParamGridBuilder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going for a very small grid because of compute time\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth,[3,5]) \\\n",
    "    .addGrid(gbt.maxBins,[16,32]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossVal = CrossValidator(estimator=gbt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=3,\n",
    "                          seed=42,\n",
    "                          parallelism=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = crossVal.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now evaluating on the test set\n",
    "predictions = cvModel.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-evaluating metrics using the resulting model\n",
    "acc_eval = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "f1_eval = MulticlassClassificationEvaluator(metricName='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating metrics\n",
    "acc = acc_eval.evaluate(predictions)\n",
    "f1 = f1_eval.evaluate(predictions)\n",
    "print(f'Accuracy: {acc:<4.2%} F-1 Score: {f1:<4.3f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the best performing model\n",
    "for key, value in cvModel.getEstimatorParamMaps()[np.argmax(cvModel.avgMetrics)].items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the hyperparameters that performed the best on this smaller dataset:\n",
    "- maxDepth:3 - All trees in the ensemble were limited to a depth of 3\n",
    "- maxBins: 16 - All continuous features are binned together for the algorithm to evaluate where to split the data. 16 is the highest number of bins that this algorithm can make per feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the best estimator's feature_importances_\n",
    "importances = cvModel.bestModel.featureImportances.toArray()\n",
    "\n",
    "# Grabbing the indices that would sort the feature importances according to their importance rating\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "# Creating a features array\n",
    "features = np.array(features)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.barh(range(len(indices)), importances[indices],\n",
    "       color=\"b\", align=\"center\")\n",
    "plt.yticks(range(len(indices)), features[indices])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like these were the most important features when predicting churn:\n",
    "- Average song plays per session\n",
    "- Total thumbs ups\n",
    "- Most consecutive days not playing songs\n",
    "\n",
    "Additionally, it looks like a few error-based metrics and total listens were completely useless for this dataset. This makes sense since there were no errors in this smaller dataset. I've chosen to keep them as part of the workflow in case the larger dataset has errors that could be useful to model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot more features can be engineered from user activity, such as thumbs-ups per day/week/month, thumbs-ups to thumbs-downs ratio, etc. \n",
    "\n",
    ">***Feature engineering can improve results better than simply optimizing one algorithm.***\n",
    "\n",
    "Thus, further work can be done extracting more features from our transactional user data to improve our predictions!\n",
    "\n",
    "Once a model is created, perhaps it can be deployed in production and run every x-amount of days or hours. Once we have a prediction on a user that is likely to churn, we have an opportunity to intervene!\n",
    "\n",
    "To evaluate how well this hypothetically deployed model does, we can run some proof-of-concept analysis and not intervene on its predictions for a given testing period. If the users it predicts will churn end up churning at a higher rate than the average user tends to churn, this can indicate that our model is working correctly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Result\n",
    "For anyone interested, after the workflow finished on AWS, the GBT classifier greatly improved its accuracy and f-1 score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWS Output](./images/AWSresult.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are almost both 0.9!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
